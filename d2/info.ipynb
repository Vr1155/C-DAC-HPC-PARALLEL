{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Overview\n",
    "- HPC Job Scheduling\n",
    "- Peformance analysis\n",
    "- Debugging\n",
    "- Profiling\n",
    "- Petascale and Exascale Computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Cluster Terminology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## What is a Job\n",
    "- Job \n",
    "    - User's program/name of an executable\n",
    "    - input data and parameters\n",
    "    - environment variables\n",
    "    - required libraries\n",
    "    - descriptions of computing resources required\n",
    "- Job Script\n",
    "    - Formal specification\n",
    "    - identifies an application to run along with its input data and env variables\n",
    "    - requests computing resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Local Resource Manager (LRM) / Batch System\n",
    "\n",
    "- Job Scheduler or Workload Manager\n",
    "    - Identifies jobs to run, selects the resources for the job, and decides when to run the job.\n",
    "- Resource manager\n",
    "    - identifies the compute resources and keeps track of their usage and feeds back this info to the workload manager.\n",
    "- Execution manager\n",
    "    - job initiation and start of execution is co-ordinated by the execution manager of the batch system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Scheduling\n",
    "- The LRM  is responsible for receiving and parsing job script.\n",
    "- if a job cannot be executed immediately, it is added to a queue. \n",
    "\n",
    "## Job Scheduling Policies:\n",
    "- FCFS\n",
    "- Multi-priority queues\n",
    "- Back-filing\n",
    "- Fair-share\n",
    "- Premptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Job execution in Compute Nodes.\n",
    "- User -> Job script -> Head Node/Login NOde -> Network Switch -> Compute Nodes\n",
    "- Job script: Selects the appropriate node for job execution. exec and input files are copied to compute nodes and job is started.\n",
    "- Login Node: Monitor's the status of job submitted. (Its not supposed to run anything, only compute nodes run execution)\n",
    "- After execution, Input and Output files are written to user specified location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Local Resource Manager - SLURM\n",
    "- When you login to HPC cluster, you land on login nodes.\n",
    "    - Login nodes are not meant to run jobs.\n",
    "    - These are used to submit jobs to Compute Nodes.\n",
    "- To submit job on the cluster, you need to write a scheduler job script.\n",
    "- SLURM - Simple Linux Utility for Resource Mangement.\n",
    "- It is a local manager that provides a framework for job queues, allocation of computer nodes, and the start and execution of jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLURM Components\n",
    "- client commands list\n",
    "- compute node daemons\n",
    "    - slurmd\n",
    "- controller daemons\n",
    "    - slurmctld\n",
    "    - secondary slurmctld\n",
    "    - slurmdbd\n",
    "- data base\n",
    "- other clusters (option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## SLURM Commands\n",
    "The list and descriptions of the mostly used Slurm commands (refer ppt).\n",
    "- sbatch\n",
    "- squeue -> info on job queue\n",
    "- sinfo -> info on all nodes, partition, and their availabilty\n",
    "- scancel\n",
    "- scontrol \n",
    "- sacct\n",
    "- srun\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLURM: Sample Job Script for Serial Jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## SLURM: Sample Job Script for Parallel Jobs on GPUs\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -N 1                                // number of nodes\n",
    "\n",
    "#SBATCH --ntasks-per-node=40                // number of cores per node\n",
    "\n",
    "#SBATCH --output=3mm.out                    // name of output file\n",
    "\n",
    "#SBATCH --error=3mm.err                     // name of error file\n",
    "\n",
    "#SBATCH --time=01:00:00                     // time required to execute the program\n",
    "\n",
    "#SBATCH --gres=gpu:2                        // request use of GPUs on compute nodes\n",
    "\n",
    "#SBATCH --partition=gpu                     // partition or queue name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export OMP_NUM_THREADS=40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commands executed in Param Utkarsh:\n",
    "- sinfo\n",
    "- squeue\n",
    "- ls /tmp/slurm-samples\n",
    "- cp -r /tmp/slurm-samples ~/\n",
    "- cd slurm-samples\n",
    "- sbatch test1.slurm\n",
    "- ls -lrt\n",
    "- squeue -u \\<username\\>\n",
    "- sbatch test2.slurm\n",
    "- cp test1.slurm test-sleep.slurm\n",
    "- vim test-sleep.slurm\n",
    "- (Add line \"sleep 120\" so that this job will sleep for 120 seconds).\n",
    "- sbatch test-sleep.slurm\n",
    "- squeue -u \\<username\\>\n",
    "- scontrol show job \\<jobid\\>\n",
    "- scancel \\<jobid\\>\n",
    "- sacct - u \\<username\\>\n",
    "- srun --nodes=1 --ntasks-per-node=1 --time=00:05:00 --pty bash\n",
    "- (This srun command will queue a job with jobid and will wait till resources are allocated to that jobid)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
