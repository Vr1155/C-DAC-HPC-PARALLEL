{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f390fa58-a833-4c5a-b7b2-e37e956747c1",
   "metadata": {},
   "source": [
    "# HPC and Parallel Programming Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763988a-9efc-4d4e-8c9f-e846232363bd",
   "metadata": {},
   "source": [
    "In serial computing get performance improvement by:\n",
    "- increasing ALU width\n",
    "- increase clock frequency\n",
    "- use pipelining\n",
    "- improved compilers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff3289c-fe93-4e07-9161-788fafe91428",
   "metadata": {},
   "source": [
    "Throughput = amount of work done in one unit of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73a71f-3118-4299-8387-40b43d51b3f3",
   "metadata": {},
   "source": [
    "Seqential systems are slow. They have only one core."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051dbcf9-312d-42e0-a8f6-a2f12a4f8a7c",
   "metadata": {},
   "source": [
    "## Parallel Architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28617a76-783b-4b54-8cb0-3058be22e6fc",
   "metadata": {},
   "source": [
    "## Vector Processors\n",
    "- Able to run mathematical operations on multiple data elements simultaneously.\n",
    "- Each \"Vector Register\" is a register file with N registers, each holding one element in the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa37381-dfe6-4691-9824-ffbf7230ba0a",
   "metadata": {},
   "source": [
    "## Super Scalar Processors\n",
    "- lets say there are 4 instructions like fetch, decode, execute, and writeback.\n",
    "- a particular unit of data has to go through each different processing unit sequentially.\n",
    "- Therefore, here super scalar processors help speed up the processing.\n",
    "- Instruction level parallelism with a processor.\n",
    "- Instruction level parallelism is also called data parallelism\n",
    "- This can be achieved by data pipelining, i.e: fetch, decode, execute, and writeback all happen at same time but on different data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6704b-0f10-42a2-bcbe-9c93cc7f1c6c",
   "metadata": {},
   "source": [
    "## MultiCore chips.\n",
    "- multiple cores in one chip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96e727-aa14-425e-9281-0399373e7aaa",
   "metadata": {},
   "source": [
    "## MultiChip module\n",
    "- multiple chips in one module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6346ecdb-4035-49e0-b12f-7c01f3a2e54e",
   "metadata": {},
   "source": [
    "## Accelerators\n",
    "- Field-Programmable Gate Arrays\n",
    "    - A computer chip that can rewrire itself for a given task.\n",
    "    - Can be programmed with hardware description languages such as VHDL or Verilog.\n",
    "- General Purpose GPU\n",
    "    - General-purpose computing on graphics processing units (GPGPU)\n",
    "    - CUDA/OpenCL programming environment.\n",
    "    \n",
    "Latest gpgpu card from Nvidia is Nvidia V100\n",
    "- inside there are int, float units, tensor core, and many more compute units.\n",
    "- refer the architecture diagram in the ppt for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ad90c-4ddc-42fc-bfc9-7fefe978f272",
   "metadata": {},
   "source": [
    "## CPU vs GPU\n",
    "- Number of cores (ALU) is higher in GPU.\n",
    "- Control unit and cache is much smaller in GPU.\n",
    "- A GPU is need CPU to work, it cannot work independently, it works like an accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a847f929-5b4f-479b-9741-0c5982c38f66",
   "metadata": {},
   "source": [
    "## Cluster Computing\n",
    "- Tightly coupled computers that work together closely as though they are a single computer.\n",
    "- PARAM PADMA AND PARAM UTKARSH\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e0ef4-cabf-4b12-b774-f272c946992e",
   "metadata": {},
   "source": [
    "## Basics of Cluster computing:\n",
    "- Processing\n",
    "    - Symmetric multiprocessing (SMP)\n",
    "    - Massively Parallel Processing (MPP)\n",
    "- Interconnects\n",
    "- Operating Systems\n",
    "- Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb0c5d-2a86-4778-92a9-adbac474accb",
   "metadata": {},
   "source": [
    "## Other parallel systems:\n",
    "- Distributed Computing\n",
    "    - Parts of a program are run at the same time on separate computers communicating over a network.\n",
    "- Grid Computing\n",
    "    - Aggregation, sharing of distributed heterogeneous system.\n",
    "- Cloud Computing\n",
    "    - On-demand available service - IaaS, PaaS, SaaS\n",
    "    - Pay-as-you-use over the Internet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56d2ad-c739-475f-a143-e7d261280b24",
   "metadata": {},
   "source": [
    "## Memory Architecture\n",
    "- Uniform Memory Access (UMA):\n",
    "    - Example - Symmetric Multiprocessor (SMP)\n",
    "    - Identical processors\n",
    "    - Equal access and access times to memory\n",
    "    - Sometimes called Cache Coherent UMA (CC-UMA).\n",
    "    - Cache Coherency is accomplished at the hardware level.\n",
    "    - Contention - as more CPUs are added, competition for access to the bus leads to a decline in performance.\n",
    "    - Thus, scalability ~ 32 processors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625edb0f-aa15-401b-9ba1-3d2695455f9f",
   "metadata": {},
   "source": [
    "- Non-Uniform Memory Access (NUMA):\n",
    "    - Not all processors have equal access time to all memories.\n",
    "    - Memory access across is slower.\n",
    "    - If cahce coherency is maintained then called CC-NUMA\n",
    "    - Designed to overcome scalability\n",
    "    - supports upto 1024 processors\n",
    "    - Processors directly attached to memory module...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b9148-3304-4d4d-a9e8-12c2fd4b3c7d",
   "metadata": {},
   "source": [
    "- Distributed Memory Architecture\n",
    "    - Processors have their own local memory and operates independently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab3b53-b2c8-457b-8b7a-f76af23d5ccc",
   "metadata": {},
   "source": [
    "- Hybrid architecture\n",
    "    - each node has its own memory and they are connected through memory bus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675e53d-2ca4-415c-b3c4-a616003bbe0e",
   "metadata": {},
   "source": [
    "## Parallel Computing Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a5112-1325-404d-a3eb-e49a7092955e",
   "metadata": {},
   "source": [
    "### Flynn's Taxonomy\n",
    "- SISD: Single Instruction Single Data\n",
    "- SIMD: Single Instruction Multiple Data\n",
    "- MISD: Multiple Instruction Single Data\n",
    "- MIMD: Multiple Instruction Multiple Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b08b0-41eb-4012-921f-70ccfa61f0e8",
   "metadata": {},
   "source": [
    "#### SISD\n",
    "- simple sequential programming\n",
    "#### SIMD\n",
    "- single instruction is applied to multiple data units.\n",
    "- each computation is done by different processing unit after taking data from a data pool and single instruction from instruction pool.\n",
    "#### MISD\n",
    "- data is same but different instructions are there.\n",
    "- different instructions are taken from instruction pool and single data is taken from data pool.\n",
    "#### MIMD\n",
    "- multiple different data are fetched from data pool\n",
    "- multiple different instructions are from instruciton pool\n",
    "- different processing unit are utilized for parallel processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
