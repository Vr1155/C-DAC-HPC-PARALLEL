{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "647a9a43-1ecf-40d2-a7b0-14525bf0a18a",
   "metadata": {},
   "source": [
    "# Parallel Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8712e-ec84-4d6f-b894-145b87474eef",
   "metadata": {},
   "source": [
    "## What is the difference between Sequential vs Parallel Programming?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f068a-0a37-4e67-9ff6-1b286205e3a6",
   "metadata": {},
   "source": [
    "## Important points:\n",
    "- identifying inherent parallelism\n",
    "- partitioning/decomposition \n",
    "- dependency analysis\n",
    "- programming paradigms\n",
    "- performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c7b91-7078-41df-939e-b21011ce3b8e",
   "metadata": {},
   "source": [
    "## Identifing inherently parallelism\n",
    "- Identifying the parallizable parts in a program is a crucial step in parallel programming.\n",
    "- It involves analyzing the program's structure, algorithms and data dependencies to identify portions that can be executed concurrently.\n",
    "- Some techniques:\n",
    "    - Manual Inspection\n",
    "    - Static Code Analysis\n",
    "    - Code Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521f0f2-0087-4a6d-917b-ebc3a4309784",
   "metadata": {},
   "source": [
    "- Code Profiling tool: gprof is a code profiling tool from GNU which can be used for performance analysis.\n",
    "- Code debugging tools: gdb and valgrind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba976d2c-cce3-43ec-98a2-3bb6e4155fc8",
   "metadata": {},
   "source": [
    "## Decomposition:\n",
    "- decomposition in parallel programming refers to the process of breaking a problem or data into subproblems or smaller pieces of data.\n",
    "\n",
    "## Data Decomposition:\n",
    "- Data partition\n",
    "- Task creation\n",
    "\n",
    "## Recurisive Decomposition:\n",
    "- divide a problem into smaller subproblems of the same type, until a base case s reached.\n",
    "- Divide and conquer principle.\n",
    "\n",
    "## Exploratory Decomposition:\n",
    "- dynamically expore and adapt the decomposition strategy based on insights gained during execution of parallel program.\n",
    "- for eg: N-Queens problem.\n",
    "\n",
    "\n",
    "## Speculative Decomposition:\n",
    "- speculatively executing multiple independent tasks or subproblems concurrently, without knowing for certan if all of them are necessary or will lead to a valid solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54b9cb-d146-421c-b1b6-b290229901c1",
   "metadata": {},
   "source": [
    "## Data dependency:\n",
    "- data dependencies must be recognized and ensure that the computations are executed in correct order.\n",
    "- because, order of statements in the code may not match the execution order.\n",
    "- This can be solved by:\n",
    "    - isolating dependencies within individual tasks or\n",
    "    - coordinating the execution of multiple tasks.\n",
    "- When they exists across parallel taks, we do explicit coordination in execution of tasks. (i.e. synchronization techniques in OpenMP or OpenMPI)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786b091-de28-4983-a6c7-5ee34dccf7c9",
   "metadata": {},
   "source": [
    "- An example of race condition is 2 threads trying to access the same location in memory and at least one of the threads writes to the shared location.\n",
    "- We might receive a different outcome everytime the program runs if there is no mechanism.\n",
    "- Therefore, there must be some mechanism to avoid race conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0276ad-de2b-4b76-a1b2-a1509a452655",
   "metadata": {},
   "source": [
    "### types of dependencies\n",
    "- Read after write [True dependency] (unparallizeable)\n",
    "- write after read (resolved by variable renaming or register renaming)\n",
    "- write after write [Output dependency] (can be resolved by register renaming)\n",
    "- looped dependencies\n",
    "    - loop carried dependencies - occurs between accesses across different loop iterations\n",
    "    - loop independent dependencies - occurs between accesses in the same loop iteration\n",
    "    - Examples\n",
    "- Control flow dependencies:\n",
    "    - execution of next instruction depends on outcome of branch or control flow statement.\n",
    "    - restrict parallel execution because outcome of one branch determines which instructions will be executed next.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdebbee0-ee40-4097-afa5-9d15da126f91",
   "metadata": {},
   "source": [
    "## Choice of Algorithm:\n",
    "- The choice of algorithm impacts data dependency.\n",
    "- Example: to compute the sum of an input array.\n",
    "    - serial sum is non-parallizeable\n",
    "    - adding adjacent pairs (merge algorithm in merge sort) is parallelizeable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e25e9-91ff-4716-8039-4824017a602d",
   "metadata": {},
   "source": [
    "## When do processors communicate?\n",
    "- To share data between different parallel tasks or processes.\n",
    "- for eg:\n",
    "    - scatter, gather, broadcast, reduction in OpenMPI.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949aa089-2b9b-4251-a821-c171726942c4",
   "metadata": {},
   "source": [
    "## Synchronization overhead\n",
    "- in case of barrier, the slowest task determines the time for execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f75945-53f6-4916-b420-69096e6a88d2",
   "metadata": {},
   "source": [
    "## Granularity:\n",
    "- Granularity (or grain size) of a task is a measured of the amount of work (or computation) which is performed by that task.\n",
    "- Or granualrity = computation time/communication time.\n",
    "- Two types of granularity:\n",
    "    - fine-grained Granularity:\n",
    "        - Smaller chunk size\n",
    "        - Relatively less computation work between communication events.\n",
    "        - Better with shared memory model.\n",
    "        - Less communication overhead.\n",
    "        - Better choice while handling load balancing.\n",
    "        - for eg: OpenMPI\n",
    "    - coarse-grained\n",
    "        - bigger chunk size\n",
    "        - computation > communication\n",
    "        - tasks completes more work with relatively less communication/synchronization events.\n",
    "        - well suited for distributed memory system.\n",
    "        - more comms overhead.\n",
    "        - for eg: OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc554c-a8f8-4411-8e51-f1c03e9c3874",
   "metadata": {},
   "source": [
    "## Peformance optimization\n",
    "- Load Balancing\n",
    "- Dynamic Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ae001-2926-4a45-8a11-47af9dc3d1b0",
   "metadata": {},
   "source": [
    "## Data Locality:\n",
    "- Spatial locality -> occurs wehn different data memeters that are located near to each other \n",
    "- Temporal locality -> different data is used very close in time.\n",
    "- better locality ---> less cache misses\n",
    "- An important form of spatial locality occurs when all elements that appear on one cache line are used together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37390f6-229c-4f9b-b03a-68e6fe53ed6d",
   "metadata": {},
   "source": [
    "## Amdahl's law:\n",
    "- According to Amdahl's Law, the speedup of a program is limited by the fraction of the program that cannot be parallelized and applications can almost never be completely parallelized.\n",
    "- S(p) = 1 / ((1-P) + (P/N))\n",
    "- Even if you have an infinite number of processors (n) ther will always be limit to achievable speedup based on the serial portion of the program (1-p)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9503831-5a41-43b2-8361-14c3b890a1ff",
   "metadata": {},
   "source": [
    "- Three numericals on speedup by using Amdahl's law and formula.\n",
    "- Note: Amdahl's law does not assume any overheads.\n",
    "- In real world system, you will encouner overheads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ff0e8-1d9e-4f46-b887-a76bffa084a6",
   "metadata": {},
   "source": [
    "- Linear vs theoretical vs realistic speedup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc07ab0-a2a5-4dfa-8952-80de20b97492",
   "metadata": {},
   "source": [
    "## Approach for Parallelizing Matrix-Matrix Addition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c13d4e-8f0a-4627-9b8a-922ccf530e29",
   "metadata": {},
   "source": [
    "- Divide the matrix among 4 processors. \n",
    "- You can divide in any ways. (row, col or block)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e14e7-6de4-4a81-8ed2-13814b52e33c",
   "metadata": {},
   "source": [
    "## Parallel programming paradigms:\n",
    "- On a singl emachine with shared memory:\n",
    "    - Pthreads\n",
    "    - OpenMP 3\n",
    "- On machines connected via network and have no shared memory:\n",
    "    - MPI\n",
    "    - PGAS\n",
    "- Accelerators, offload tasks from CPU to it\n",
    "    - CUDA\n",
    "    - OpenACC/OpenMP4\n",
    "    - OpenCL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce93c0-26fa-4cd9-9d09-406825892788",
   "metadata": {},
   "source": [
    "- Example for pthread, OpenMP, CUDA, OpenACC, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d52ed-f7f5-4463-803f-92d8063a9f6c",
   "metadata": {},
   "source": [
    "## Summary of approaches for developing parallel software.\n",
    "- Understand the problem.\n",
    "- Determine if its parallizeable.\n",
    "- Identify the hotspots (where real work is being done)/\n",
    "- Identify the bottlenecks (reduce or eliminate unnecessary slow areas)\n",
    "- Identify appropriate algos\n",
    "- Investigate inhibitors to parallelism. one such inhibitor is data dependencies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
