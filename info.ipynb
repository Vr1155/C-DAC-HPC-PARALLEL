{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Overview\n",
    "- HPC Job Scheduling\n",
    "- Peformance analysis\n",
    "- Debugging\n",
    "- Profiling\n",
    "- Petascale and Exascale Computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Cluster Terminology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## What is a Job\n",
    "- Job \n",
    "    - User's program/name of an executable\n",
    "    - input data and parameters\n",
    "    - environment variables\n",
    "    - required libraries\n",
    "    - descriptions of computing resources required\n",
    "- Job Script\n",
    "    - Formal specification\n",
    "    - identifies an application to run along with its input data and env variables\n",
    "    - requests computing resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Local Resource Manager (LRM) / Batch System\n",
    "\n",
    "- Job Scheduler or Workload Manager\n",
    "    - Identifies jobs to run, selects the resources for the job, and decides when to run the job.\n",
    "- Resource manager\n",
    "    - identifies the compute resources and keeps track of their usage and feeds back this info to the workload manager.\n",
    "- Execution manager\n",
    "    - job initiation and start of execution is co-ordinated by the execution manager of the batch system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Scheduling\n",
    "- The LRM  is responsible for receiving and parsing job script.\n",
    "- if a job cannot be executed immediately, it is added to a queue. \n",
    "\n",
    "## Job Scheduling Policies:\n",
    "- FCFS\n",
    "- Multi-priority queues\n",
    "- Back-filing\n",
    "- Fair-share\n",
    "- Premptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Job execution in Compute Nodes.\n",
    "- User -> Job script -> Head Node/Login NOde -> Network Switch -> Compute Nodes\n",
    "- Job script: Selects the appropriate node for job execution. exec and input files are copied to compute nodes and job is started.\n",
    "- Login Node: Monitor's the status of job submitted. (Its not supposed to run anything, only compute nodes run execution)\n",
    "- After execution, Input and Output files are written to user specified location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Local Resource Manager - SLURM\n",
    "- When you login to HPC cluster, you land on login nodes.\n",
    "    - Login nodes are not meant to run jobs.\n",
    "    - These are used to submit jobs to Compute Nodes.\n",
    "- To submit job on the cluster, you need to write a scheduler job script.\n",
    "- SLURM - Simple Linux Utility for Resource Mangement.\n",
    "- It is a local manager that provides a framework for job queues, allocation of computer nodes, and the start and execution of jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLURM Components\n",
    "- client commands list\n",
    "- compute node daemons\n",
    "    - slurmd\n",
    "- controller daemons\n",
    "    - slurmctld\n",
    "    - secondary slurmctld\n",
    "    - slurmdbd\n",
    "- data base\n",
    "- other clusters (option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## SLURM Commands\n",
    "The list and descriptions of the mostly used Slurm commands (refer ppt).\n",
    "- sbatch\n",
    "- squeue -> info on job queue\n",
    "- sinfo -> info on all nodes, partition, and their availabilty\n",
    "- scancel\n",
    "- scontrol \n",
    "- sacct\n",
    "- srun\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLURM: Sample Job Script for Serial Jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## SLURM: Sample Job Script for Parallel Jobs on GPUs\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -N 1                                // number of nodes\n",
    "\n",
    "#SBATCH --ntasks-per-node=40                // number of cores per node\n",
    "\n",
    "#SBATCH --output=3mm.out                    // name of output file\n",
    "\n",
    "#SBATCH --error=3mm.err                     // name of error file\n",
    "\n",
    "#SBATCH --time=01:00:00                     // time required to execute the program\n",
    "\n",
    "#SBATCH --gres=gpu:2                        // request use of GPUs on compute nodes\n",
    "\n",
    "#SBATCH --partition=gpu                     // partition or queue name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export OMP_NUM_THREADS=40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commands executed in Param Utkarsh:\n",
    "- sinfo\n",
    "- squeue\n",
    "- ls /tmp/slurm-samples\n",
    "- cp -r /tmp/slurm-samples ~/\n",
    "- cd slurm-samples\n",
    "- sbatch test1.slurm\n",
    "- ls -lrt\n",
    "- squeue -u \\<username\\>\n",
    "- sbatch test2.slurm\n",
    "- cp test1.slurm test-sleep.slurm\n",
    "- vim test-sleep.slurm\n",
    "    - (Add line \"sleep 120\" so that this job will sleep for 120 seconds).\n",
    "- sbatch test-sleep.slurm\n",
    "- squeue -u \\<username\\>\n",
    "- scontrol show job \\<jobid\\>\n",
    "- scancel \\<jobid\\>\n",
    "- sacct - u \\<username\\>\n",
    "- srun --nodes=1 --ntasks-per-node=1 --time=00:05:00 --pty bash\n",
    "    - (This srun command will queue a job with jobid and will wait till resources are allocated to that jobid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Module Utility\n",
    "- How to set Environment?\n",
    "    - use module command \n",
    "- some important commands:\n",
    "    - module avail -> To see the available software installed on HPC system.\n",
    "        - list of precompiled\n",
    "    - module list\n",
    "    - module load\n",
    "    - module unload\n",
    "    - module purge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Commands executed:\n",
    "- mpicc\n",
    "- module avail | grep openmpi\n",
    "    - This will show different versions of a software if available\n",
    "- module load openmpi/4.1.1\n",
    "- mpicc\n",
    "- which mpicc\n",
    "- echo $PATH\n",
    "    - openmpi/4.1.1 is added to path\n",
    "- echo $LD_LIBRARY_PATH\n",
    "    - library files are displayed\n",
    "- module unload openmpi/4.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferring files between local machine and HPC cluster\n",
    "- When a user wishes to transfer data from their local system (laptopo/desktop) to HPC system. They can just use \"scp\" command on their terminal.\n",
    "- The command shown below can be used for effecting file transfer \"scp -r \\<path to the local data directory\\> \\<your username\\>@\\<IP of ParamUtkarsh\\>:\\<path to directory on HPC where to save the data\\>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLURM Job Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Commands executed:\n",
    "- sbatch array-jobs.slurm\n",
    "- sacct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Dependent Jobs/Workflows\n",
    "- A standard example of this is a workflow in which the output from one job is used as the input to the next.\n",
    "- Useful when you need to run multiple jobs in a particular order.\n",
    "\n",
    "                    jobA\n",
    "                /           \\\n",
    "            jobB            jobC\n",
    "                \\           /\n",
    "                    jobD\n",
    "\n",
    "- Instructor says that jobB and jobC are running in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Commands executed:\n",
    "- sbatch JobA.slurm\n",
    "- sbatch -d afterok:\\<jobidA\\> JobB.slurm\n",
    "- sbatch -d afterok:\\<jobidA\\> JobC.slurm\n",
    "- sbatch -d afterok:\\<jobidB\\>:\\<jobid\\> JobD.slurm\n",
    "\n",
    "- /tmp/commands\n",
    "    - this file will store all commands executed till now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Output of /tmp/commands:\n",
    "\n",
    "paramutkarsh.cdac.in\n",
    "\n",
    "ssh user@14.139.1.74 -p 4422\n",
    "\n",
    "ls /tmp/slurm-samples\n",
    "\n",
    "cp -r /tmp/slurm-samples ~/\n",
    "\n",
    "cd ~/slurm-samples\n",
    "\n",
    "ls\n",
    "\n",
    "sbatch test1.slurm\n",
    "\n",
    "ls -lrt\n",
    "\n",
    "squeue -u <username>\n",
    "\n",
    "\n",
    "sbatch test2.slurm\n",
    "\n",
    "cp test1.slurm test-sleep.slurm\n",
    "\n",
    "vi test-sleep.slurm\n",
    "\n",
    "sbatch test-sleep.slurm\n",
    "\n",
    "srun --nodes=1 --ntasks-per-node=1 --time=00:05:00 --pty bash\n",
    "\n",
    "mpicc\n",
    "\n",
    "module avail | grep openmpi\n",
    "\n",
    " module load openmpi/4.1.1\n",
    "\n",
    "which mpicc\n",
    "\n",
    "echo $PATH\n",
    "echo $LD_LIBRARY_PATH\n",
    "\n",
    "module unload openmpi/4.1.1\n",
    "\n",
    "sbatch array-jobs.slurm\n",
    "\n",
    "sbatch -d afterok:<jobidA> JobB.slurm\n",
    "\n",
    "sbatch -d afterok:<jobidA> JobC.slurm\n",
    "\n",
    "sbatch -d afterok:<jobidB>:<jobidC> JobD.slurm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speedup\n",
    "\n",
    "- speedup = (Sequential execution time)/(parallel execution time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Amdahl's Law:\n",
    "- S(n) = <= 1/(((1-f) + (f/n))) <= 1/f (for large values of n)\n",
    "- S(n) = theoritical speedup with n processors\n",
    "- f = serial fraction\n",
    "- n =  number of processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gustafson's Law:\n",
    "- S(n) = n + (1-n)f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Granularity:\n",
    "- granularity = size of computation between syncrhonization points.\n",
    "    - coarse -> heavyweight processes + IPC (Interprocess communication like MPI)\n",
    "    - Fine -> Instruction level (eg. SIMD)\n",
    "    - Medium -> Threads + [message passing + shared memory]\n",
    "- Computation to Communication Ratio\n",
    "    - (Computation time)/(Communication time)\n",
    "    - Increasing this ratio is often a key to good efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication Overhead\n",
    "- Comms overhead = time (measured in instructions) a zero-byte message consumes in a process.\n",
    "    - Measure time spent on communication that cannot be spent on computation.\n",
    "- Overlapped Message - portion of message lifetime that can occur concurrently with computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Parallel vs Serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Parallel bugs\n",
    "- Race conditions\n",
    "- Deadlocks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eg of deadlock and code samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Overview of GDB\n",
    "- GNU Debugger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Preparing your program\n",
    "- Additional step when compiling program\n",
    "- Add a -g option to enable built-in debugging support (which gdb needs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commands executed:\n",
    "- cd slurm-samples\n",
    "- gdb gdb-test\n",
    "- list\n",
    "    - list the source code and line numbers\n",
    "- break 6\n",
    "    - sets breakpoint at line number 6\n",
    "- run \"cdacbangalore\"\n",
    "- next\n",
    "- backtrace\n",
    "- info frame\n",
    "- x &a1\n",
    "- print a1\n",
    "- quit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenMP debugging with GDB\n",
    "- Thread-specific commands in GDB:\n",
    "    - info_thread\n",
    "    - thread\n",
    "    - thread \\<thread_no\\>\n",
    "    - thread apply\n",
    "    ....\n",
    "\n",
    "## Debuggers\n",
    "- GDB\n",
    "- LLDB\n",
    "- TotalView\n",
    "- DDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "- Profiling is useful for performance analysis and measurement of a software applications to identify and diagnose performance issues or bottlenecks.\n",
    "- various profilers are available like vtool (see ppt for more)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Petascale and Exascale Computing:\n",
    "- Cores, Rmax (PFlop/s), Rpeak (PFlop/s), Power(kW) \n",
    "- Rmax = maximum achieved on HPL benchmark\n",
    "- Rpeak = theoretical maximum performance\n",
    "- AIRAWAT - PARAM Siddhi - AI of 8.5 Petaflops is the fastest supercomputer in India.\n",
    "- ranked at no. 90 position in \"Top500\" supercomputer list - November 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Challenges\n",
    "- power consumption\n",
    "- scalability\n",
    "- heterogeneity\n",
    "- resilience\n",
    "- programming methodologies and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## C-DAC software for system software\n",
    "- ParaDE -> parallel desktop environment\n",
    "- CAPC -> CDAC auto parallizing compiler\n",
    "- CHAP -> CDAC HPC Application Profiler\n",
    "- SuPrakashan -> monitoring and management solution\n",
    "- HPC Dashboard.\n",
    "\n",
    "**All these softwares are available only to PARAM systems**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
