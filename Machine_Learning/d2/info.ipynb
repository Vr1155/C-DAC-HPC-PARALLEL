{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification problems are an important category of problems where outcome variable takes discrete values.\n",
    "- Primary objective is to predict the probability of an observation belonging to a class, known as class probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of classification..\n",
    "- Classification problems with binary outcomes are called binary classification.\n",
    "- Classification problems with multiple outcomes are called multinomial classification.\n",
    "- Techniques used for solving classification problems:\n",
    "    1. Logistical regression\n",
    "    2. Classificaion trees\n",
    "    3. Discriminant analysis\n",
    "    4. Neural networks\n",
    "    5. Support vector machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes classificaiton: A simple example.\n",
    "- Suppose we have the following patient data, which consists of a single feature (blood glucose level), and a class label 0 for non-diabetic and 1 for diabetic.\n",
    "- put blood glucose level on x-axis and class label on y-axis.\n",
    "- when we plot the graph, we get a sigmoid curve which connects all the points.\n",
    "- Here line y = 0.5 is the classifier. It breaks the sample space into two different classes, diabetic and non-diabetic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes classification.\n",
    "- Dataset: diabetes dataset\n",
    "- The diabetes dataset used in this exercise is based on data originally collected by National Institute of Diabetes and Digestive and Kidney Diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NOTE: you can load a dataset from some other location on internet using wget command:\n",
    "```python\n",
    "# load the training dataset:\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/ml-basics/diabetes.csv\n",
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "diabetes.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identification of x and y: all attributes are feature vectors except \"Diabetic\".\n",
    "- Here, \"Diabetic\" is a label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Feature x and label y\n",
    "```python\n",
    "\n",
    "# Separate features and labels\n",
    "features = [\"Pregnancies\", \"PlasmaGlucose\", \"DiastolicBloodPressure\", TricepsThickness, SerumInsuline, BMI, DiabetesPedigree, Age]\n",
    "lebel = 'Diabetic'\n",
    "X, y = diabetes[features].values, diabetes[label].values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the dataset into training and testing sets.\n",
    "```python\n",
    "\n",
    "\n",
    "    X_train ... = train_test_split(X,y, test_size=0.30, random_state=0)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Building model\n",
    "    - regularization rate = learning rate\n",
    "    - if learning rate is too high, then we might not get accurate result.\n",
    "    - if learning rate is too low, then the learning process can take a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correct deductions (class 0)\n",
    "    - TN = True Negative\n",
    "    - TP = True Positive\n",
    "- Incorrect deduction (class 1)\n",
    "    - FN = False Negative\n",
    "    - FP = False Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall determines the total quantity of true positives (some may be wrongly deducted).\n",
    "    - Recall : TP / (TP + FN) = of all the cases that are positive, how many did model identify?\n",
    "    - Recall is the **quantity** determination of true positives.\n",
    "- Precision determines the total number of TP (all are correctly determined)\n",
    "    - Precision: TP / (TP + FP) = of all cases that the model predicted to be positive, how many actually are positive?\n",
    "    - Precision is the **quality** determination of true positives.\n",
    "- F1-Score: It is a collection of Recall and Precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that in this case, Precision and Accuracy are two different things.\n",
    "- Precision = Dart thrower is throwing darts at some common spot. It may or may not be close to bullseye.\n",
    "- Accuracy = Dart thrower is throwing darts very close to bullseye, but there might be some variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained model:\n",
    "- Calculating the classification report to determine precision, recall, F1-score, etc..\n",
    "```python\n",
    "\n",
    "    refer slides for code\n",
    "    ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculating the overall precision, recall.\n",
    "\n",
    "```python\n",
    "\n",
    "    refer slides for code\n",
    "    ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Therefore, there are 4 important metrics to analyze in classification task:\n",
    "    1. Accuracy\n",
    "    2. Precision\n",
    "    3. Recall\n",
    "    4. F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why decision tree?\n",
    "- In order to solve linearly inseparable problems, or in order to classify linearly inseparable problems, decision tree can be used.\n",
    "- For linear separability: https://en.wikipedia.org/wiki/Linear_separability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using decision tree:\n",
    "- Task1: Selecting Informative Attributes.\n",
    "- Task2: Visualising the segmentation.\n",
    "- Task3: Trees as set of rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Informative Attributes:\n",
    "- Instructor displays a photo of stick figures of different body shapes (squares and circles) with yes or no above them.\n",
    "- Attributes: \n",
    "    - head-shape: square, circular\n",
    "    - body-shape: rectangular, oval\n",
    "    - body-color: gray, white\n",
    "- Target variable:\n",
    "    - write-off: Yes, No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "- Entropy is used for binary classification problem, which is having relationship with probability as shown in figure.\n",
    "- Instructor draws a graph of an upside down parabola, such that, y=0 when x=0 and x=1, and y = 1 when x = 0.5.\n",
    "- In this graph, y-axis is E(S) and x-axis is probability from 0 to 1.\n",
    "- E(S) = {1 @ p=0.5, 0 @ p=0 & p=1} \n",
    "- For binary classification: E(S) = -1 * P(Y)*log2(P(Y)) - P(N)*log2(P(N))  [for c = 2]\n",
    "- for eg: S : {4P, 2F}\n",
    "    - E(S) = -(4/6)*log2(4/6) - (2/6)*log2(2/6) = 0.918\n",
    "    - You can also use online entropy calculator and scientific calculator to do this calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "- For more info see: https://en.wikipedia.org/wiki/Information_gain_(decision_tree)\n",
    "- Look at section \"Another Take on Information Gain, with Example\" for more easy to understand explaination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:\n",
    "- A research is trying to identify the root node to design a decision tree classifier to classify the students based on scores greater than or equal to 50% which is a pass and less than 50% which is a fail. The data is given in the below table.\n",
    "- Refer the slides for data table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Questions:\n",
    "1. Find the entropy of the column.\n",
    "    - Solution: S : {4P, 2F}\n",
    "    - E(S) = -(4/6)*log2(4/6) - (2/6)*log2(2/6) = 0.918\n",
    "    - You can also use online entropy calculator and scientific calculator to do this calculation.\n",
    "\n",
    "2. Calculate the information gain (IG) for the parameter 'Attendance %'.\n",
    "    - Solution:\n",
    "    - Instructor draws a table for calculating information gain:\n",
    "    - Attendance: LT70 LT70 LT70 LT70 MT70 MT70\n",
    "    - Target C  : F    F    P    P    P    P\n",
    "    - For root node LT70, S = {4P, 2F}\n",
    "    - E(S) is entropy of parent, E(S1) is the entropy of child 1, E(S2) is the entropy of child 2.\n",
    "    - Therefore, formula for Information Gain for parameter 'Attendance %' (A) is given by:\n",
    "    - IG(A, E(S)) = E(S) - (Sv1/S)*E(S1) - (Sv2/S)*E(S2)\n",
    "        - Here, S = no. of class in Parent\n",
    "        - Sv1 = no. of class in child 1\n",
    "        - Sv2 = no. of class in child 2.\n",
    "    - so, here, S = 6, Sv1 = 4, Sv2 = 2.\n",
    "    - E(S) = -(4/6)*log(4/6) - (2/6)*log(2/6) = 0.91\n",
    "    - E(S1) = -(2/4)*log(2/4) - (2/4)*log(2/4) = 1\n",
    "    - E(S2) = -(2/2)*log(2/2) - 0 = 0\n",
    "    - therefore, IG(A, E(S)) = E(S) - (Sv1/S)*E(S1) - (Sv2/S)*E(S2) = 0.91 - 4/6 * 1 - 2/6 * 0 = 0.25\n",
    "\n",
    "\n",
    "2a. question: Calculate the IG value of \"% of marks\" as selected attribute for the given problem.\n",
    "\n",
    "- % M = 30 45 50 70 75 85\n",
    "- T C = F  F  P  P  P  P\n",
    "- S : {4P, 2F}\n",
    "- S1: {2F, 0P}\n",
    "- S2: {4P, 0F}\n",
    "- IG: 0.918\n",
    "- do this as homework.\n",
    " \n",
    "3. Which of the following can be accepted as root node.\n",
    "- % of marks\n",
    "- % of attendance\n",
    "- No. of assignments completed\n",
    "- No. of hours studied.\n",
    "    - Solution:\n",
    "    - Instructor says that students must try all and calculate IG values for each and every one of them and select the one with best IG value.\n",
    "    - Do this as homework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Segmentations: An Example.\n",
    "- Instructor shows a decision tree of Balance vs age and probabilities of write-off or no write-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees as a set of rule\n",
    "- IF(Balance < 50K) AND (Age < 50) THEN Class = Write-off\n",
    "- IF(Balance < 50K) AND (Age >= 50) THEN Class = No Write-off\n",
    "- IF(Balance >= 50K) AND (Age < 45) THEN Class = Write-off\n",
    "- IF(Balance >= 50K) AND (Age >= 45) THEN Class = No Write-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the previous problem, assume that % of marks is selected as root node based on IG value, write down tree as a set of rules for that.\n",
    "- Trees as a set of rule:\n",
    "- IF (Marks >= 50%) THEN Class = Pass\n",
    "- IF (Marks < 50%) THEN Class = Fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sigmoid function:\n",
    "- P(Y) = 1/(1+e^(-x))\n",
    "    - x = independent variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Different types of probabilities:\n",
    "    - Marginal: P(A)\n",
    "    - Joint: P(A U B)\n",
    "    - Conditional: P(A|B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula for conditional probability:\n",
    "- P(A|B) = P(A â‹‚ B) / P(B) - [A & B are dependent in nature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "- P(A|B) = P(A and B) / P(B) - [1]\n",
    "- P(B|A) = P(A and B) / P(A)\n",
    "- P(A and B) = P(B|A) * P(A) - [subsititute in eq 1]\n",
    "- P(A|B) = ( P(B|A) * P(A) ) / P(B)\n",
    "- Bayes' Theorem: P(A|B) = ( P(B|A) * P(A) ) / P(B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
