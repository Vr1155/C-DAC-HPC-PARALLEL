{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate DeepLearning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every cell in our body takes some input and produces some output.\n",
    "- Input can be food and output can be some work.\n",
    "- Similarly, brain cells which are called neurons, take some input and produce some output in the form of electrical signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Artificial Neural Network tries to model this functionality of biological neural networks.\n",
    "- Artificial Neural Network takes some input [x1, x2, x3, x4, ..., xn]\n",
    "- It has a set of \"Weights\" [w1, w2, w3, w4, ..., wn] that are used for processing the input.\n",
    "- There is also something called \"bias\" which is used to modify the input in certain way.\n",
    "- Finally the Neuron produces an output y which is essentially a function of inputs xi, weights wi and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep learning is an advanced form of machine learning that tries to emulate the way the human brain learns.\n",
    "- In your brain, you have nerve cells called neurons, which are connected to one another by nerve extensions that pass electrohemical signals through the network.\n",
    "- (refer slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When there's more than one input value, x is considered a vector with elements named x1, x2, and so on.\n",
    "- Associated with each x value is a weight (w), which is used to strengthen or weaken the effect of the x value to simulate learning.\n",
    "- Additionally a bias (b) input is added to enable fine-grained control over the network.\n",
    "- During the training process, the w and b values will be adjusted to tune the network so that it \"learns\" to produce correct outputs.\n",
    "- The neuron itself encapsulates a function that calculates a **weighted sum of x, w, and b**.\n",
    "- This function is in turn enclosed in an **activation function** that constraints the result (often to a value between 0 and 1) to determine whether or not the neuron passes an output onto the next layer of neurons in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In simple terms, a machine learning model is a function that calculates y (the label) from x (the features): f(x) = y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Classification problem:- Learning of Penguin species system.\n",
    "- Input: For example, suppose your observation consists of some measurements of a penguin.\n",
    "- Specifically, the measurements are: The length of penguin's bill (x1), the depth of the penguin's bill (x2), the length of the penguin's flipper (x3), and the penguin's weight (x4).\n",
    "- So, the features (x) are a vector of four values, or mathematically, x = [x1, x2, x3, x4].\n",
    "- Output: Let's suppose that the label we're trying to predict (y) is the species of the penguin, and that there are three possible species it could be: 0-Adelie, 1-Gentoo, and 2-Chinstrap.\n",
    "- As you can see, it is a classification problems where input should be some characteristics of penguin and output is the one of the three species of penguin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This system needs 4 features for measurement, hence 4 neurons are needed input layer.\n",
    "- The proposed system is intended to classify 3 penguin species, hence 3 neurons are needed at the output layer.\n",
    "- Based upon requirement of deep learning, the number of hidden layers are decided (minimum 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The deep neural network model for the classifier consists of multiple layers of artificial neurons. \n",
    "- The training process for a deep neural netowkr consists of multiple iterations, called **epochs**. \n",
    "- For the first epoch, you start by assigning random initialization values for the weight (w) and bias (b) values.\n",
    "- Then the process is as follows:\n",
    "    1. Features for data observations with known label values are submitted to the input layer. Generally, these observations are grouped into batches (often referred to as **mini-batches**).\n",
    "    2. The neurons then apply their function, and if activated, pass the result onto the next layer until the output layer produces a prediction.\n",
    "    3. Calculation of **loss** is done, which is the amount of variance between the predicted and true values.\n",
    "    4. Revised value for the weights and bias values are calculated to reduce the loss, and these adjustments are **back propagated** to the neurons in the network layers.\n",
    "    5. The next epoch repeats the batch training forward pass with the revised weights and bias values, hopefully improving the accuracy of the model (by reducing the loss).\n",
    "- **Remember these 5 steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A closer look at loss functions and backpropagation\n",
    "- The previous description of deeplearning training process mentioned that the loss from the model is calculated and used to adjust the weight and bias values. How exactly does this work?\n",
    "### Calculating loss\n",
    "- Suppose one of the samples passed through the training process contains features of an Adelie specimen (class 0). \n",
    "- The correct output from the network would be [1, 0, 0].\n",
    "- New suppose that the output produced by the network is [0.4, 0.3, 0.3].\n",
    "- Comparing these, we can calculate an absolute varaince for each element (in other words, how far is each predicted value away from what it should be) as [0.6, 0.3, 0.3].\n",
    "- In reality, since we're actually dealing with multiple observations, we typically aggregate the variance - for example by squaring the individual variance values and calculating the mean, so we end up with a single average loss value like 0.18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "### Optimizers:\n",
    "- The loss is calculated using a funciton, which operates on the results from the final layer of the network, which is also a function.\n",
    "- The final layer of network operates on the outputs from the previous layers, which are also functions.\n",
    "- So in efect, the entire model from the input layer right through to the loss calculation is just one big nested function.\n",
    "- Functions have a few really useful characteristics, including:\n",
    "    - You can conceptualize a function as a plotted line comparing its output with each of its variables.\n",
    "    - You can use differential calculus to calculate the derivative of the function at any point with respect to its variables.\n",
    "- Lets take the first of these capabilities. \n",
    "- We can plot the line of function to show how an individual weight value compares to loss, and mark on that line the point where the current weight value matches the current loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (refer slides)\n",
    "- There are two ways to implement backpropagation:\n",
    "    - manual way\n",
    "    - systematic way using optimizers\n",
    "- There are multiple commonly used optimization algorithms, including:\n",
    "    - Stochastic Gradient Descent (SGD)\n",
    "    - Adaptive Learning Rate (ADADELTA)\n",
    "    - Adaptive Momentum Estimation (Adam)\n",
    "    - and others.\n",
    "- All of which are designed to figure out how to adjust the weights and biases to minimize loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Minima\n",
    "- The best solution of learning is called as global minima.\n",
    "- The Global Minima is nothing but the values of w, b when loss function's output is least.\n",
    "- Sometime due to improper speed of learning, we may miss global minima.\n",
    "- To avoid this, we must carefully select a parameter called as learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "- (refer slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of deep learning algorithm for Penguin species classification case study (Code from python notebook (PenguinDLSklearn.ipynb)):\n",
    "1. Observe that data is available in two different fiels training and test.\n",
    "2. As the feature vector species is in categorical form, so we need to convert this into a numeric value.\n",
    "3. In order to perform training operations, we need to perform standardization/scaling operations with numpy.\n",
    "4. Now we need to apply this data to a neural network model.\n",
    "5. Now we need to validate and test this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN):\n",
    "- Definition of Convolution in mathematics: https://en.wikipedia.org/wiki/Convolution\n",
    "- Convolution is a mathematical operation that allows the merging of two sets of information. \n",
    "- In the case of CNN, convolution is applied to the input data to filter the information and produce a feature map. \n",
    "- This filter is also called a kernel, or feature detector, and its dimensions can be, for example, 3x3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNNs is actually a form of deep learning, It converts image related input data into some feature value.\n",
    "- While you can use deep learning models for any kind of machine learning, they're particularly useful for dealing with data that consists of large arrays of numeric values - such as images.\n",
    "- Machine learning models that work with images are the foundation for an area artificial intelligence called **Computer Vision**, and deep learning techniques have been responsible for driving amazing advances in this area over recent years.\n",
    "- (refer slides)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers in a CNN\n",
    "- CNNs consist of multiple layers, each performing a specific task in extracting features or predicting labels.\n",
    "- **Convolutional layers:** One of the principal layer types is a convolutional layer that extracts important features in images.\n",
    "- A convolutional layer works by applying a filter to images.\n",
    "- The filter is defined by a kernel that consists of a matrix of weight values.\n",
    "- For example, a 3x3 filter might be defined like this:\n",
    "<pre>\n",
    "[\n",
    " 1 -1  1\n",
    "-1  0 -1\n",
    " 1 -1  1\n",
    "]\n",
    "</pre>\n",
    "- Black and white images are represented by 1 matrix.\n",
    "- Color images are represented by 3 matrices (For each of the RGB values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layers\n",
    "- Due to size of filter kernel we use padding value of 0.\n",
    "- Rectified Linear Unit (ReLU) is an activation function whose output is same as input for positive values and its output is zero for negative value.\n",
    "- Image -> Convolution -> ReLU -> (-ve w = 0, +ve w = as it is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers:\n",
    "- After extracting feature valeus from images, pooling (or downsampling) layers are used to reduce the number of feature values while retains the key differentiating features that have been extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening layer:\n",
    "- A flattening layer is used to flatten the feature maps into a vector of values that can be used as input to a fully connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected CNN:\n",
    "1. Images are fed into a convolutional layer. In this case, there are two filters, so each images produces two feature maps.\n",
    "2. The feature maps are passed to a pooling layer, where a 2x2 pooling kernel reduces the size of the feature maps.\n",
    "3. A dropping layer randomly drops some of the feature maps to help prevent overfitting.\n",
    "4. A flattening layer takes the remaining feature map arrays and flattens them into a vector.\n",
    "5. The vector elements are fed into a fully connected network, which generates the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN using Keras in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Refer jupyter notebook from google colab link\n",
    "- api is similar to what we've seen in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whenever you are using PyTorch, it is advisable to use GPU.\n",
    "- In google Colab, go to runtime -> change runtime type -> select a GPU.\n",
    "- If you want access to premium GPUs? you might have to purchase google's \"Compute Units\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Notice that pytorch uses OOP, but the idea is still the same.\n",
    "- We create a class called Net, this is an empty neural network, aka just a skeleton.\n",
    "- we define some hyperparameters like\n",
    "    - hidden layer = 20\n",
    "    - number of epoch = 40\n",
    "    - learning rate = 0.01\n",
    "- Then we choose a optimizer and loss function\n",
    "- we use something like:\n",
    "    - torch.Tensor(xtrain).float()\n",
    "    - torch.Tensor(ytrain).long()\n",
    "- This was done because we need to do type conversion before we use pytorch dataframe.\n",
    "- Then there is a for loop that does the training.\n",
    "- refer the jupyter notebooks for more info."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
