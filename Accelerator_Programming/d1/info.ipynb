{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerator Programming:\n",
    "\n",
    "## What is a Accelerator:\n",
    "    \n",
    "- Hardware Acceleration is the use of computer hardware designed to perform specific functions more efficiently, when compared to software running on a general-purpose central processing unit (CPU).\n",
    "\n",
    "- Any software running on GPCPU can run on custom-made hardware.\n",
    "\n",
    "- This custom-made hardware made for hardware acceleration are called Accelerators.\n",
    "\n",
    "\n",
    "## Types of Accelerators:\n",
    "- GPU\n",
    "- FPGA \n",
    "- ASIC\n",
    "\n",
    "## GPU:\n",
    "- GPU = Graphics Processing Unit.\n",
    "- Originally created for accelerating graphics rendering in video games.\n",
    "- Now they are used in accelerating any task that can be parallelized.\n",
    "- GPUs can have several cores (usually thousands).\n",
    "- CPUs are optimized for serial computational workloads while GPUs are optimized for parallel computational workloads.\n",
    "- Instructor says that concurrently means sequentially, the keyword is \"PARALLELY\".\n",
    "- CPU has to be faster because its supposed to do a variety of tasks, which includes loading OS, dealing with RAM, device drivers, etc.\n",
    "    \n",
    "## FPGA:\n",
    "- FPGA = Field Programmable Gate Arrays (FPGAs)\n",
    "- Used to offload computationally intensive workloads from the main CPU, allowing it to focus on other tasks.\n",
    "- FPGAs are modular. which means we can change its architecture as per the liking of the user.\n",
    "- This is because they are \"Programmable\", this is unlike GPUs where we cannot change the architecture.\n",
    "- Applications: financial modelling, scientific simulations, encryption/decryption, network traffic management, etc.\n",
    "- financial trading and algorithmic trading where low latency is crucial.\n",
    "\n",
    "## FPGA Architecture:\n",
    "- refer to the slides.\n",
    "- DSP blocks = Digital Signal Processing blocks\n",
    "- RAM blocks = Random Access Memory blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CPU:\n",
    "- ALUs are the cores of both CPU and GPU.\n",
    "- A Quad core CPU has 4 ALUs, each core will have control block and L1 Cache (The fastest cache inside the computer).\n",
    "- Instructor says that the farther the cache is from the CPU the more power computer has to expend to access that memory. \n",
    "- This is why L1 cache is fastest.\n",
    "- Similarly there is L2 Cache which is a little further from cores and L3 Cache is again more further from L2 Cache.\n",
    "- And finally there is DRAM which is furthest memory block from cores inside the CPU.\n",
    "- Caches stores some small amout of data in the form of pages.\n",
    "- If there is a cache miss, then CPU has to go to RAM.\n",
    "\n",
    "## CPU strengths:\n",
    "- large memory\n",
    "- fast clock speed\n",
    "\n",
    "## CPU weakness:\n",
    "- low memory bandwidth\n",
    "- low performace/watt\n",
    "- expensive cache miss\n",
    "\n",
    "## GPU strenghts:\n",
    "- high bandwidth memory\n",
    "- more compute resources\n",
    "- latency tolerant via parallelism\n",
    "- high performance/watt\n",
    "## GPU weakness:\n",
    "- relatively low memory capacity\n",
    "- low per thread performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Nvidia GPU Tesla V100:\n",
    "- Tensor core can parallely execute matrix arithmetic (like addition, subtraction, multiplication, etc.)\n",
    "- SFU and LDU is communicating data between Core so GPU and textual storage unit at the bottom.\n",
    "- GPC = Graphics Processing cluster, this contains many GPU cards.\n",
    "- tex = textual processing unit.\n",
    "- HBM = High Bandwidth Memory\n",
    "- NVLink is used for Nvidia's propreitary card for Nvidia compiler for nvCC for running the CUDA codes.\n",
    "- CUDA code can be identified from \"__global__\" in the source code.\n",
    "\n",
    "## Other usecases of GPUs:\n",
    "- GPUs are also used by Nvidia and other companies to make autonomous driving cars.\n",
    "- This is because that task involves 360 degree videos and to speedup the processing it will require parallel computation.\n",
    "- This is a realtime and critical application. Therefore, speedup is crucial and hence GPUs are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Processing Unit (TPU):\n",
    "- A TPU is an accelerator designed by Google for accelerating machine learning (ML) workloads.\n",
    "- They are custom-designed ASICs (Application-Specific Integrated Circuits) optimized for ML algorithms.\n",
    "- They provide high performace and low power consumption.\n",
    "\n",
    "## TPU architecutre:\n",
    "- refer slides.\n",
    "\n",
    "## TPU core components:\n",
    "- matrix multiplier unit \n",
    "- Unified buffer\n",
    "- activation Unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Use case of Accelerators:\n",
    "- Use of specialized hardware (Accelerator) to speed up work, often with parallel processing.\n",
    "\n",
    "## Programming on Accelerators:\n",
    "- GPU:\n",
    "    - CUDA (proprietary by Nvidia)\n",
    "    - OpenACC (Open Source by Nvidia, helps developers develop application of their choice)\n",
    "    - SYCL (New entrant, can be used on any kind of hardware)\n",
    "- FPGA:\n",
    "    - SYCL\n",
    "- ASICs:\n",
    "    - SYCL\n",
    "\n",
    "- We will learn about SYCL more in-depth in these sessions.\n",
    "- Instructor says, if you switch from Nvidia to AMD, CUDA and OpenACC wont work due to vendor lock-in.\n",
    "- SYCL's motivation was that it helps port source code from one architecture to another.\n",
    "- Apps like \"final cut pro\" use CUDA for parallel processing of video editing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Languages:\n",
    "- CUDA:\n",
    "    - CUDA C Keyword \\_\\_global\\_\\_ indicates that a function:\n",
    "        - runs on the device.\n",
    "        - called from a host.\n",
    "- nvcc is a compiler for CUDA code.\n",
    "- nvcc splits source file into host and device components.\n",
    "- NVIDIA's compiler handles device function like kernel()\n",
    "- Host compiler handles host functions like main().\n",
    "- OpenACC is based on C, its old and deprecated, everyone is moving to SYCL which is based on C++.\n",
    "- GCC will be the default compiler on host machine and nvcc will be the default compiler on device.\n",
    "- nvcc is unofficially based on GCC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Example of CUDA code:\n",
    "\n",
    "```c\n",
    "__global__ void kernel(void)\n",
    "{\n",
    "    ...\n",
    "}\n",
    "\n",
    "int main(void){\n",
    "    kernel<<< input parameters >>>();\n",
    "    printf(\"Hello, World!\\n\");\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Flow of Accelerator Programming:\n",
    "1. Copy data from main memory to GPU memory.\n",
    "2. CPU instructs the processing to GPU.\n",
    "3. GPU executes paralle in each core.\n",
    "4. Copy the results from the GPU memory to the main memory.\n",
    "\n",
    "- see slides for the image on processing flow on CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Another example of CUDA code:\n",
    "- code to calculate sin^2 and cos^2 and doing addition using GPU is on the slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenACC:\n",
    "- OpenACC stands for Open Accelerators, is a programming standard for parallel computing.\n",
    "- It allows programmers to simplify the process of offloading computations to GPUs while still using familiar programming languages like C, C++ and Fortran.\n",
    "- to specify that --openacc -targets\n",
    "- refer to the slides for full code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## OpenMP\n",
    "- A new directive to offload to accelerators is introduced in openmp4.\n",
    "- Clauses for data transfer is similar to that of OpenACC.\n",
    "- sample openmp code is in slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## OpenCL:\n",
    "- OpenCL standard API is a KHRONOS implementation.\n",
    "- Leverage CPU's, GPU's and other processors and DSP's to accelerate parallel computing.\n",
    "- kernel function is in a different file and host code (written in C/C++) will bein a different file.\n",
    "- When both files are compiled together, kernel code will be loaded on accelerator.\n",
    "- allows you to write accelerated portable code across various devices and architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## SYCL:\n",
    "- SYCL is a KHRONOS standard, it provides a high level abstraction layer over C++.\n",
    "- It extends C++ in two key ways:\n",
    "    - device discovery\n",
    "    - device control\n",
    "- It allows you to write both host and device code in same source file.\n",
    "- It allows you to write portable code across different devices and architectures.\n",
    "- Helps avoid vendor-lock.\n",
    "- Short and lengthy sample code available on slides.\n",
    "<pre>\n",
    "                        SYCL\n",
    "            /             |             \\\n",
    "        GPU             FPGA            ACICs\n",
    "         |\n",
    "        Nvidia/AMD\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Now we will login to PARAM Utkarsh and learn how to write code and execute programs on that supercomputer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some info missing here, refer slides for more detail on SYCL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Sample SYCL code for adding two arrays:\n",
    "```cpp\n",
    "\n",
    "int main(){\n",
    "    int i;\n",
    "    int *dataA, *dataB, *dataC;\n",
    "    queue Q;\n",
    "    dataA = new int[N];\n",
    "    dataA = new int[N];\n",
    "    dataA = new int[N];\n",
    "\n",
    "    for(i = 0; i < N; i++){\n",
    "        dataA[i] = 90;\n",
    "        dataB[i] = 10;\n",
    "    }\n",
    "\n",
    "    buffer<int, 1> buff_a(dataA, range<1>(N));\n",
    "    buffer<int, 1> buff_b(dataB, range<1>(N));\n",
    "    buffer<int, 1> buff_c(dataC, range<1>(N));\n",
    "\n",
    "    Q.submit([&] (handler &cgh) {\n",
    "        auto acc_A buff_a.get_access<access::mode::read>(cgh);\n",
    "        auto acc_B buff_b.get_access<access::mode::read>(cgh);\n",
    "        auto acc_C buff_c.get_access<access::mode::read_write>(cgh);\n",
    "\n",
    "        cgh.parallel_for(range<1>(N), [=](id<1> indx){\n",
    "            acc_C[indx] = acc_A[indx] + acc_B[indx];\n",
    "        });\n",
    "    });\n",
    "\n",
    "    auto C = buff_c.get_access<access::mode::read>();\n",
    "\n",
    "    for(i = 0; i < N; i++){\n",
    "       cout << C[i] << \" \";\n",
    "    }\n",
    "    cout << std::end;\n",
    "    delete[] dataA;\n",
    "\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## SYCL implementations:\n",
    "- DPC++ (For PARAM UTKARSH we will use this)\n",
    "- ComputeCpp\n",
    "- triSYCL\n",
    "- hipSYCL\n",
    "- neoSYCL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Data:\n",
    "- Learn about buffer/accessor data of managing data.\n",
    "- learn about buffer and accessor\n",
    "- learn to access data in the kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tommorow we will work on parade-ide (ide for param utkarsh) and we will write some SYCL after getting the login credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParaDE:\n",
    "- make sure you get the login credentials before you login.\n",
    "- go to paramutkarsh.cdacb.in:8447/#/parade-ide\n",
    "- click on new project.\n",
    "- for parallel projects click on \"Hybrid with C [HC]\"\n",
    "- give project name as \"hello.cpp\"\n",
    "- click on src folder.\n",
    "- create a file named \"hello.cpp\"\n",
    "- you will see the file has been opened in editor.\n",
    "- write a simple hello world program in cpp.\n",
    "- inorder to switch your compiler, go to settings -> switch compiler and select from the list of compilers.\n",
    "- For C++ go to \"CPU C compilers\" -> g++.\n",
    "- For SYCL go to icpx.\n",
    "- click on compile icon (gears) to generate the output file\n",
    "- execute by clicking on play button icon, you will see Cluster Job submission modal.\n",
    "- you can specify command line arguments, partitions, number of GPUs per node, node count, tasks per node, whether some specific nodes required, etc.\n",
    "- After submitting, you will see a popup with the job ID of your submission.\n",
    "- You can see output in Output tab\n",
    "- see job details in My Jobs tab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Instructor executes some other code, following is displayed in the output tab:\n",
    "    - Platforms: Intel FPGA Emulation Platform for OpenCL.\n",
    "    - device: Intel FPGA Emulation Device\n",
    "    - Platforms: Intel OpenCL\n",
    "    - device: Intel Xeon Platinum 8268 CPU @ 2.90 GHz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Tommorow we will have a demo session on SYCL and how to run codes on CPU as well as GPU."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
